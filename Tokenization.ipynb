{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68960ec7-0835-434d-93e3-6a0e8f7c78c0",
   "metadata": {},
   "source": [
    "##  Tokenizing a Short Story for LLM Training (Educational Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35421a93-dac7-4c9e-b5ac-846fa0b1e075",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; color: #155724;\">\n",
    "\n",
    "In this notebook, we aim to tokenize a 20,479-character short story, The Verdict by Edith Wharton, into a sequence of individual words and special characters to simulate preprocessing steps used in Large Language Model (LLM) training. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd61648-fa37-4a14-9f41-0a9c3afa0193",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d1ecf1; padding: 15px; border-radius: 5px; color: #0c5460;\">\n",
    "\n",
    "While LLMs are typically trained on gigabytes of text data from millions of documents, we use this shorter text sample for educational purposes and to ensure quick runtime on consumer hardware.We begin by reading the entire file into memory and printing the character count and a sample of the content for context. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2923594-4e56-478e-85eb-ba28058fa250",
   "metadata": {},
   "source": [
    "## Step 1: Reading the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c05b8f6-7857-43da-82e5-4ae94327b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "First 100 characters:\n",
      " I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# Load the raw text file\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Print total number of characters and a sample\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"First 100 characters:\\n\", raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc15a2-d5e0-42c6-ab10-24ec48694be3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff3cd; padding: 15px; border-radius: 5px; color: #856404;\">\n",
    "\n",
    "To tokenize the text, we use Python’s re (regular expressions) module, splitting the text into words and punctuation marks using a carefully designed pattern: re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text). This pattern ensures that we retain all meaningful tokens—including punctuation and whitespace—as separate elements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3f307-c6ad-4db8-8959-5d72e44ea9e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e2e3e5; padding: 15px; border-radius: 5px; color: #383d41;\">\n",
    "\n",
    "We then remove empty strings and pure whitespace from the resulting list using a combination of strip() and a filtering condition. The result is a list of clean, discrete tokens that can be used as input for embedding generation or other downstream NLP tasks. While whitespace is discarded in our approach for simplicity, this decision is task-dependent: retaining whitespace may be important for applications involving structured or indentation-sensitive text (e.g., programming code).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279cc3c3-8c54-41b4-82b6-c78c7147dae4",
   "metadata": {},
   "source": [
    "##  Step 2: Basic Tokenization Using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fca4b5c-7a96-4ee6-9b1a-18d1fab1299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 tokens:\n",
      " ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split on various punctuation and whitespace characters, keeping them as separate tokens\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "\n",
    "# Remove empty strings and pure whitespace tokens\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# Show the first 30 tokens for inspection\n",
    "print(\"First 30 tokens:\\n\", preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469ac50-1e90-423a-a5fe-d820c08bf85e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; color: #155724;\">\n",
    "\n",
    "This simplified tokenizer demonstrates key principles of tokenization and prepares us for transitioning to pre-built tokenizers from libraries like Hugging Face Transformers, spaCy, or SentencePiece, which handle more complex linguistic phenomena and are optimized for modern LLM workflows.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
