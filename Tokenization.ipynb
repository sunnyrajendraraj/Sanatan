{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68960ec7-0835-434d-93e3-6a0e8f7c78c0",
   "metadata": {},
   "source": [
    "##  Tokenizing a Short Story for LLM Training (Educational Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35421a93-dac7-4c9e-b5ac-846fa0b1e075",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; color: #155724;\">\n",
    "\n",
    "In this notebook, we aim to tokenize a 20,479-character short story, The Verdict by Edith Wharton, into a sequence of individual words and special characters to simulate preprocessing steps used in Large Language Model (LLM) training. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd61648-fa37-4a14-9f41-0a9c3afa0193",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d1ecf1; padding: 15px; border-radius: 5px; color: #0c5460;\">\n",
    "\n",
    "While LLMs are typically trained on gigabytes of text data from millions of documents, we use this shorter text sample for educational purposes and to ensure quick runtime on consumer hardware.We begin by reading the entire file into memory and printing the character count and a sample of the content for context. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2923594-4e56-478e-85eb-ba28058fa250",
   "metadata": {},
   "source": [
    "## Step 1: Reading the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c05b8f6-7857-43da-82e5-4ae94327b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20541\n",
      "First 100 characters:\n",
      " This is a sample text data we took for the training purpose.\n",
      "\n",
      "I HAD always thought Jack Gisburn rat\n"
     ]
    }
   ],
   "source": [
    "# Load the raw text file\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Print total number of characters and a sample\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(\"First 100 characters:\\n\", raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc15a2-d5e0-42c6-ab10-24ec48694be3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff3cd; padding: 15px; border-radius: 5px; color: #856404;\">\n",
    "\n",
    "To tokenize the text, we use Python’s re (regular expressions) module, splitting the text into words and punctuation marks using a carefully designed pattern: re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text). This pattern ensures that we retain all meaningful tokens—including punctuation and whitespace—as separate elements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3f307-c6ad-4db8-8959-5d72e44ea9e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e2e3e5; padding: 15px; border-radius: 5px; color: #383d41;\">\n",
    "\n",
    "We then remove empty strings and pure whitespace from the resulting list using a combination of strip() and a filtering condition. The result is a list of clean, discrete tokens that can be used as input for embedding generation or other downstream NLP tasks. While whitespace is discarded in our approach for simplicity, this decision is task-dependent: retaining whitespace may be important for applications involving structured or indentation-sensitive text (e.g., programming code).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279cc3c3-8c54-41b4-82b6-c78c7147dae4",
   "metadata": {},
   "source": [
    "##  Step 2: Basic Tokenization Using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fca4b5c-7a96-4ee6-9b1a-18d1fab1299a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 tokens:\n",
      " ['This', 'is', 'a', 'sample', 'text', 'data', 'we', 'took', 'for', 'the', 'training', 'purpose', '.', 'I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split on various punctuation and whitespace characters, keeping them as separate tokens\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "\n",
    "# Remove empty strings and pure whitespace tokens\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# Show the first 30 tokens for inspection\n",
    "print(\"First 30 tokens:\\n\", preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469ac50-1e90-423a-a5fe-d820c08bf85e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; color: #155724;\">\n",
    "\n",
    "This simplified tokenizer demonstrates key principles of tokenization and prepares us for transitioning to pre-built tokenizers from libraries like Hugging Face Transformers, spaCy, or SentencePiece, which handle more complex linguistic phenomena and are optimized for modern LLM workflows.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedfa48d-c9ec-4848-85cf-b8eee3946f44",
   "metadata": {},
   "source": [
    " ## Building a custom tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1e917-33ff-4692-9b4a-f99586d5cb39",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff3cd; padding: 15px; border-radius: 5px; color: #856404;\">\n",
    "\n",
    "From the preprocessed text, we constructed a vocabulary by identifying all unique tokens and assigning them integer IDs, forming the foundation for converting text into numerical form.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764aa9a-c704-43db-bdca-5c7985cc3e55",
   "metadata": {},
   "source": [
    "##  Step 1: Import and Prepare Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16097e1f-1088-4fbe-a21f-81926d5ecbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1133)\n",
      "('your', 1134)\n",
      "('yourself', 1135)\n",
      "('<|unk|>', 1136)\n",
      "('<|endoftext|>', 1137)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Assume 'preprocessed' is a list of tokens from the training data\n",
    "# Create a sorted list of unique tokens from training data\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "# Add special tokens\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "\n",
    "# Create vocabulary (token to ID) and inverse vocabulary (ID to token)\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "# Display last 5 entries in the vocabulary\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71442ae7-18fa-4bd5-8a97-90330384feef",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f8d7da; padding: 15px; border-radius: 5px; color: #721c24;\">\n",
    "\n",
    "Initially, we created a basic tokenizer class that could encode input text into token IDs using the vocabulary and decode it back to the original text. However, this version failed when encountering unknown words—tokens that did not appear in the training text—leading to KeyError exceptions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299ba58-0b9c-418c-a659-6dfcd6ef1a0a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #d4edda; padding: 15px; border-radius: 5px; color: #155724;\">\n",
    "\n",
    "To solve this, we enhanced the basic tokinenizer by introducing two special tokens: <|unk|> to represent unknown tokens and <|endoftext|> to mark the end of a sentence or document boundary. These were appended to the vocabulary, and logic was added in the encode() method to replace any out-of-vocabulary token with <|unk|>, ensuring robustness against new or unseen input. Additionally, we appended <|endoftext|> to every input to mimic GPT-like tokenization, which is critical when training on multiple documents. The decode() method was also refined to cleanly join tokens into readable sentences, fixing spacing issues before punctuation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c01338-23cc-4bbd-a72d-67b9cb2f4f3d",
   "metadata": {},
   "source": [
    "##  Step 2: Tokenizer Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe20a9d4-2366-4e35-b734-28a4c735e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        # Split the input text into tokens (words, punctuation, spaces)\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        \n",
    "        # Replace unknown tokens with <|unk|>\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" \n",
    "            for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        # Convert tokens to token IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        # Append end-of-text token\n",
    "        ids.append(self.str_to_int[\"<|endoftext|>\"])\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        # Convert token IDs back to string tokens\n",
    "        tokens = [self.int_to_str.get(i, \"<|unk|>\") for i in ids]\n",
    "\n",
    "        # Join tokens into a string\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        # Fix spacing before punctuation\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75082fcb-3847-477d-8123-3274c694f3e1",
   "metadata": {},
   "source": [
    "## Step 3 : Instantiate and Test the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95da91e2-30f4-47f0-bac2-178aeff3b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "# Test it\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ad103-b248-4710-b8cf-66431ff67cd4",
   "metadata": {},
   "source": [
    "## Step 4: Encode and Decode the Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d217b7fb-7a62-4d46-8cb7-97149ecb0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1136, 5, 356, 1132, 629, 978, 10, 1137, 55, 992, 959, 987, 723, 992, 1136, 7, 1137]\n",
      "Decoded Text: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", ids)\n",
    "\n",
    "# Decode\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cddca7-0404-4a21-a8cd-4254f6f0bbf6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e2e3e5; padding: 15px; border-radius: 5px; color: #383d41;\">\n",
    "\n",
    "Overall, this tokenizer provides a foundational understanding of how text is transformed into machine-readable formats for LLMs, and how even simple rule-based methods must gracefully handle exceptions, unknowns, and formatting nuances to be effective.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
